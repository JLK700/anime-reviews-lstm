{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer (text features extraction) + Linear Regression for Anime Reviews Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('anime_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import re\n",
    "\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "def tokenize (text):\n",
    "    # remove punctuation and numbers and set to lowercase\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletion of infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "for _, record in df.iterrows():\n",
    "    vocab.update(tokenize(record['text']))\n",
    "\n",
    "for word in list(vocab):\n",
    "    if vocab[word] < 50:\n",
    "        del vocab[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "for word in list(vocab):\n",
    "    if word in STOP_WORDS:\n",
    "        del vocab[word]\n",
    "        \n",
    "del vocab['ep']\n",
    "del vocab['eps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokenized = tokenize(text)\n",
    "    for token in tokenized[:]:\n",
    "        if token not in vocab:\n",
    "            tokenized.remove(token)\n",
    "    return ' '.join(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "'''\n",
    "    sublinear_df => is set to True to use a logarithmic form for frequency.\n",
    "    min_df => is the minimum numbers of documents a word must be present in to be kept.\n",
    "    \n",
    "'''\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(df.clean_text).toarray()\n",
    "labels = df.score.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most popular unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For category 1\n",
      "  . Most correlated unigrams:\n",
      ". cu\n",
      ". pathetic\n",
      "  . Most correlated bigrams:\n",
      ". green green\n",
      ". overall pathetic\n",
      "For category 2\n",
      "  . Most correlated unigrams:\n",
      ". terrible\n",
      ". worst\n",
      "  . Most correlated bigrams:\n",
      ". animation pleasing\n",
      ". worst anime\n",
      "For category 3\n",
      "  . Most correlated unigrams:\n",
      ". poorly\n",
      ". worst\n",
      "  . Most correlated bigrams:\n",
      ". disappointed watched\n",
      ". overall poor\n",
      "For category 4\n",
      "  . Most correlated unigrams:\n",
      ". bland\n",
      ". boring\n",
      "  . Most correlated bigrams:\n",
      ". life won\n",
      ". section analysis\n",
      "For category 5\n",
      "  . Most correlated unigrams:\n",
      ". mediocrity\n",
      ". mediocre\n",
      "  . Most correlated bigrams:\n",
      ". overall disappointing\n",
      ". second commercial\n",
      "For category 6\n",
      "  . Most correlated unigrams:\n",
      ". fair\n",
      ". chirico\n",
      "  . Most correlated bigrams:\n",
      ". episode min\n",
      ". overall fair\n",
      "For category 7\n",
      "  . Most correlated unigrams:\n",
      ". omake\n",
      ". preliminary\n",
      "  . Most correlated bigrams:\n",
      ". watch special\n",
      ". enjoyed fan\n",
      "For category 8\n",
      "  . Most correlated unigrams:\n",
      ". mediocre\n",
      ". terrible\n",
      "  . Most correlated bigrams:\n",
      ". enjoyed series\n",
      ". interested japanese\n",
      "For category 9\n",
      "  . Most correlated unigrams:\n",
      ". great\n",
      ". loved\n",
      "  . Most correlated bigrams:\n",
      ". recommend lupin\n",
      ". highly recommend\n",
      "For category 10\n",
      "  . Most correlated unigrams:\n",
      ". amazing\n",
      ". masterpiece\n",
      "  . Most correlated bigrams:\n",
      ". amazing anime\n",
      ". best anime\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for category_id in ['1', '2','3','4','5','6','7','8','9','10']:\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    \n",
    "    print(f'For category {category_id}')\n",
    "    \n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features = train_test_split(features, test_size=0.2, shuffle=False)\n",
    "valid_features, test_features = train_test_split(test_features, test_size=0.5, shuffle=False)\n",
    "\n",
    "train_labels, test_labels = train_test_split(labels, test_size=0.2, shuffle=False)\n",
    "valid_labels, test_labels = train_test_split(test_labels, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\384036\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(train_features, train_labels)\n",
    "predictions = lgr.predict(valid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.astype(int)\n",
    "valid_labels = valid_labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact accuracy: 0.27473878303626303\n",
      "Accuracy missed by one: 0.6724031960663799\n",
      "Accuracy missed by two: 0.858020897357099\n",
      "Accuracy missed by three: 0.9280885064535955\n"
     ]
    }
   ],
   "source": [
    "exact_acc = 0\n",
    "one_error_acc = 0\n",
    "two_error_acc = 0\n",
    "three_error_acc = 0\n",
    "size = len(predictions)\n",
    "\n",
    "for index, p in enumerate(predictions):\n",
    "    true_label = valid_labels[index]\n",
    "    if p == true_label:\n",
    "        exact_acc += 1\n",
    "        one_error_acc += 1\n",
    "        two_error_acc += 1\n",
    "        three_error_acc += 1\n",
    "    elif p - true_label == 1 or p - true_label == -1:\n",
    "        one_error_acc += 1\n",
    "        two_error_acc += 1\n",
    "        three_error_acc += 1\n",
    "    elif p - true_label == 2 or p - true_label == -2:\n",
    "        two_error_acc += 1\n",
    "        three_error_acc += 1\n",
    "    elif p - true_label == 3 or p - true_label == -3:\n",
    "        three_error_acc += 1\n",
    "                \n",
    "print(f'Exact accuracy: {exact_acc / size}')\n",
    "print(f'Accuracy missed by one: {one_error_acc / size}')\n",
    "print(f'Accuracy missed by two: {two_error_acc / size}')\n",
    "print(f'Accuracy missed by three: {three_error_acc / size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
