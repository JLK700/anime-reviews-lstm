{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources - https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../MyAnimeList Scraper/anime_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex który usuwa ep_XX i eps_XX i dziwny śmietnik\n",
    "\n",
    "df = df.replace(regex=r'eps [0-9]+', value='')\n",
    "df = df.replace(regex=r'ep [0-9]+', value='')\n",
    "df = df.replace(regex=r'[0-9]+[a-zA-z]+', value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].replace(regex=r'[0-9]+', value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "'''\n",
    "    sublinear_df => is set to True to use a logarithmic form for frequency.\n",
    "    min_df => is the minimum numbers of documents a word must be present in to be kept.\n",
    "    \n",
    "'''\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(df.text).toarray()\n",
    "labels = df.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16275, 118288)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape\n",
    "# For 16275 reviews we extract 119386 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For category 1\n",
      "  . Most correlated unigrams:\n",
      ". nomizu\n",
      ". okazaki\n",
      "  . Most correlated bigrams:\n",
      ". shouta aoi\n",
      ". overall pathetic\n",
      "For category 2\n",
      "  . Most correlated unigrams:\n",
      ". terrible\n",
      ". worst\n",
      "  . Most correlated bigrams:\n",
      ". animation pleasing\n",
      ". worst anime\n",
      "For category 3\n",
      "  . Most correlated unigrams:\n",
      ". anos\n",
      ". worst\n",
      "  . Most correlated bigrams:\n",
      ". overall poor\n",
      ". self insertion\n",
      "For category 4\n",
      "  . Most correlated unigrams:\n",
      ". boring\n",
      ". dorei\n",
      "  . Most correlated bigrams:\n",
      ". plastic little\n",
      ". life won\n",
      "For category 5\n",
      "  . Most correlated unigrams:\n",
      ". mediocrity\n",
      ". mediocre\n",
      "  . Most correlated bigrams:\n",
      ". just brief\n",
      ". just mediocre\n",
      "For category 6\n",
      "  . Most correlated unigrams:\n",
      ". dantalian\n",
      ". neptunia\n",
      "  . Most correlated bigrams:\n",
      ". fan cgi\n",
      ". overall fair\n",
      "For category 7\n",
      "  . Most correlated unigrams:\n",
      ". omake\n",
      ". preliminary\n",
      "  . Most correlated bigrams:\n",
      ". enjoyed fan\n",
      ". watch special\n",
      "For category 8\n",
      "  . Most correlated unigrams:\n",
      ". shigofumi\n",
      ". terrible\n",
      "  . Most correlated bigrams:\n",
      ". enjoyed series\n",
      ". bit dry\n",
      "For category 9\n",
      "  . Most correlated unigrams:\n",
      ". loved\n",
      ". hige\n",
      "  . Most correlated bigrams:\n",
      ". recommend lupin\n",
      ". highly recommend\n",
      "For category 10\n",
      "  . Most correlated unigrams:\n",
      ". amazing\n",
      ". masterpiece\n",
      "  . Most correlated bigrams:\n",
      ". amazing anime\n",
      ". best anime\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for category_id in ['1', '2','3','4','5','6','7','8','9','10']:\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    \n",
    "    print(f'For category {category_id}')\n",
    "    \n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features = train_test_split(features, test_size=0.2, shuffle=False)\n",
    "valid_features, test_features = train_test_split(test_features, test_size=0.5, shuffle=False)\n",
    "\n",
    "train_labels, test_labels = train_test_split(labels, test_size=0.2, shuffle=False)\n",
    "valid_labels, test_labels = train_test_split(test_labels, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8' '8' '9' ... '6' '9' '8']\n",
      "0.21819299323909036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(train_features, train_labels)\n",
    "predictions = lgr.predict(valid_features)\n",
    "print(predictions)\n",
    "print(accuracy_score(y_true=valid_labels, y_pred=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 9, ..., 6, 9, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictions.astype(int)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9,  6, ...,  3,  7,  7])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels = valid_labels.astype(int)\n",
    "valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact accuracy: 0.21819299323909036\n",
      "Accuracy missed by one: 0.5519360786724032\n",
      "Accuracy missed by two: 0.7473878303626306\n",
      "Accuracy missed by three: 0.8518746158574063\n"
     ]
    }
   ],
   "source": [
    "exact_acc = 0\n",
    "one_error_acc = 0\n",
    "two_error_acc = 0\n",
    "three_error_acc = 0\n",
    "size = len(predictions)\n",
    "\n",
    "for index, p in enumerate(predictions):\n",
    "    true_label = valid_labels[index]\n",
    "    if p == true_label:\n",
    "        exact_acc += 1\n",
    "        one_error_acc += 1\n",
    "        two_error_acc += 1\n",
    "        three_error_acc += 1\n",
    "    elif p - true_label == 1 or p - true_label == -1:\n",
    "        one_error_acc += 1\n",
    "        two_error_acc += 1\n",
    "        three_error_acc += 1\n",
    "    elif p - true_label == 2 or p - true_label == -2:\n",
    "        two_error_acc += 1\n",
    "        three_error_acc += 1\n",
    "    elif p - true_label == 3 or p - true_label == -3:\n",
    "        three_error_acc += 1\n",
    "                \n",
    "print(f'Exact accuracy: {exact_acc / size}')\n",
    "print(f'Accuracy missed by one: {one_error_acc / size}')\n",
    "print(f'Accuracy missed by two: {two_error_acc / size}')\n",
    "print(f'Accuracy missed by three: {three_error_acc / size}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
