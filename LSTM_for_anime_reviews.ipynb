{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_Yei1l_6126"
   },
   "source": [
    "# LSTM for Anime Reviews Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8Z7AFCjS7L4d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('anime_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eP6__YN87NXm"
   },
   "outputs": [],
   "source": [
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TLZufXwwjwLr"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0ewrzBfz7Prv"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import re\n",
    "\n",
    "tok = spacy.load('en')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcG-e00l9cZp"
   },
   "source": [
    "### Deletion of infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "py9wcJ2P93bH",
    "outputId": "bf34d3f0-3a0b-432f-90fb-f1c4496d8c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 85849\n",
      "num_words after: 50123\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "for index, row in df.iterrows():\n",
    "    c.update(tokenize(row['text']))\n",
    "\n",
    "print(\"num_words before:\", len(c.keys()))\n",
    "for word in list(c):\n",
    "    if c[word] < 2:\n",
    "        del c[word]\n",
    "print(\"num_words after:\", len(c.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of vocabulary for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m5k9881m-Tr_"
   },
   "outputs": [],
   "source": [
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in c:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y6QvQ-ak_JwI"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=6000):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJmUsxjZ_XDf",
    "outputId": "479cefa2-c071-450b-d771-9ae02b7244b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df['encoded_text'] = df['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "okJ1hcLO_5vO"
   },
   "outputs": [],
   "source": [
    "#library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation & Transfer to Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XYSavsjvAlS4"
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)).to(\"cuda:0\"), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y6b-_4hiAlYw"
   },
   "outputs": [],
   "source": [
    "x = list(df['encoded_text'])\n",
    "y = list(df['score'].astype(int))\n",
    "\n",
    "for index, value in enumerate(x):\n",
    "    if value[1] == 0:\n",
    "        del x[index]\n",
    "        del y[index]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x, y, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rNcLAw-ASuhC"
   },
   "outputs": [],
   "source": [
    "for index, value in enumerate(x_train):\n",
    "    if value[1] == 0:\n",
    "        del x_train[index]\n",
    "        del y_train[index]\n",
    "\n",
    "for index, value in enumerate(x_valid):\n",
    "    if value[1] == 0:\n",
    "        del x_valid[index]\n",
    "        del y_valid[index]\n",
    "\n",
    "for index, value in enumerate(x_test):\n",
    "    if value[1] == 0:\n",
    "        del x_test[index]\n",
    "        del y_test[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M-U8VMP-lrVg"
   },
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train).to(\"cuda:0\")\n",
    "y_valid = torch.tensor(y_valid).to(\"cuda:0\")\n",
    "y_test = torch.tensor(y_test).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hLNpeZX2CiA5"
   },
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(x_train, y_train)\n",
    "valid_ds = ReviewsDataset(x_valid, y_valid)\n",
    "test_ds = ReviewsDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jH0mnYhHAlby"
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        if i % 1 == 0:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        pred = pred.to('cpu')\n",
    "        y = y.to('cpu')\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9836FdFgAleS"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Zy-qWvYlAlg9"
   },
   "outputs": [],
   "source": [
    "class LSTM_variable_input(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 11)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, (ht, ct) = self.lstm(x)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIcOblnRVeHq",
    "outputId": "da66f8a9-f7ea-4075-cbf8-fc40db2c4428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_variable_input(\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (embeddings): Embedding(50125, 700, padding_idx=0)\n",
      "  (lstm): LSTM(700, 70, batch_first=True)\n",
      "  (linear): Linear(in_features=70, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_variable_input(vocab_size, 700, 70)\n",
    "model = model.to(\"cuda:0\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORVnTXYOC9cX",
    "outputId": "31f759f5-f73e-4a5a-8cac-8f0ea06e9a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.141, val loss 1.998, val accuracy 0.226, and val rmse 2.362\n",
      "train loss 1.988, val loss 1.815, val accuracy 0.355, and val rmse 2.230\n",
      "train loss 1.857, val loss 1.693, val accuracy 0.409, and val rmse 2.036\n",
      "train loss 1.718, val loss 1.594, val accuracy 0.443, and val rmse 1.979\n",
      "train loss 1.610, val loss 1.527, val accuracy 0.474, and val rmse 1.982\n",
      "train loss 1.517, val loss 1.480, val accuracy 0.497, and val rmse 1.923\n",
      "train loss 1.441, val loss 1.448, val accuracy 0.512, and val rmse 1.903\n",
      "train loss 1.376, val loss 1.413, val accuracy 0.530, and val rmse 1.878\n",
      "train loss 1.318, val loss 1.385, val accuracy 0.544, and val rmse 1.833\n",
      "train loss 1.265, val loss 1.363, val accuracy 0.563, and val rmse 1.789\n",
      "train loss 1.232, val loss 1.344, val accuracy 0.576, and val rmse 1.803\n",
      "train loss 1.200, val loss 1.333, val accuracy 0.583, and val rmse 1.787\n",
      "train loss 1.152, val loss 1.321, val accuracy 0.593, and val rmse 1.773\n",
      "train loss 1.120, val loss 1.307, val accuracy 0.599, and val rmse 1.774\n",
      "train loss 1.091, val loss 1.314, val accuracy 0.607, and val rmse 1.715\n",
      "train loss 1.073, val loss 1.293, val accuracy 0.612, and val rmse 1.736\n",
      "train loss 1.033, val loss 1.302, val accuracy 0.620, and val rmse 1.703\n",
      "train loss 1.009, val loss 1.295, val accuracy 0.622, and val rmse 1.752\n",
      "train loss 1.001, val loss 1.292, val accuracy 0.625, and val rmse 1.702\n",
      "train loss 0.978, val loss 1.300, val accuracy 0.624, and val rmse 1.723\n",
      "train loss 0.968, val loss 1.274, val accuracy 0.637, and val rmse 1.685\n",
      "train loss 0.930, val loss 1.278, val accuracy 0.644, and val rmse 1.685\n",
      "train loss 0.924, val loss 1.283, val accuracy 0.647, and val rmse 1.675\n",
      "train loss 0.918, val loss 1.288, val accuracy 0.643, and val rmse 1.676\n",
      "train loss 0.891, val loss 1.282, val accuracy 0.651, and val rmse 1.673\n",
      "train loss 0.876, val loss 1.276, val accuracy 0.658, and val rmse 1.620\n",
      "train loss 0.860, val loss 1.272, val accuracy 0.660, and val rmse 1.631\n",
      "train loss 0.844, val loss 1.283, val accuracy 0.661, and val rmse 1.619\n",
      "train loss 0.831, val loss 1.267, val accuracy 0.662, and val rmse 1.616\n",
      "train loss 0.822, val loss 1.265, val accuracy 0.671, and val rmse 1.584\n",
      "train loss 0.825, val loss 1.270, val accuracy 0.666, and val rmse 1.598\n",
      "train loss 0.816, val loss 1.276, val accuracy 0.668, and val rmse 1.595\n",
      "train loss 0.797, val loss 1.269, val accuracy 0.668, and val rmse 1.621\n",
      "train loss 0.792, val loss 1.271, val accuracy 0.668, and val rmse 1.631\n",
      "train loss 0.799, val loss 1.263, val accuracy 0.674, and val rmse 1.601\n",
      "train loss 0.786, val loss 1.252, val accuracy 0.678, and val rmse 1.593\n",
      "train loss 0.771, val loss 1.259, val accuracy 0.680, and val rmse 1.622\n",
      "train loss 0.779, val loss 1.262, val accuracy 0.678, and val rmse 1.617\n",
      "train loss 0.769, val loss 1.264, val accuracy 0.676, and val rmse 1.627\n",
      "train loss 0.766, val loss 1.282, val accuracy 0.673, and val rmse 1.612\n",
      "train loss 0.762, val loss 1.280, val accuracy 0.679, and val rmse 1.602\n",
      "train loss 0.772, val loss 1.273, val accuracy 0.678, and val rmse 1.597\n",
      "train loss 0.759, val loss 1.268, val accuracy 0.682, and val rmse 1.587\n",
      "train loss 0.745, val loss 1.267, val accuracy 0.686, and val rmse 1.555\n",
      "train loss 0.728, val loss 1.259, val accuracy 0.683, and val rmse 1.549\n",
      "train loss 0.724, val loss 1.249, val accuracy 0.691, and val rmse 1.558\n",
      "train loss 0.714, val loss 1.253, val accuracy 0.690, and val rmse 1.556\n",
      "train loss 0.704, val loss 1.258, val accuracy 0.689, and val rmse 1.556\n",
      "train loss 0.708, val loss 1.253, val accuracy 0.696, and val rmse 1.570\n",
      "train loss 0.695, val loss 1.250, val accuracy 0.698, and val rmse 1.543\n",
      "train loss 0.687, val loss 1.252, val accuracy 0.692, and val rmse 1.550\n",
      "train loss 0.708, val loss 1.261, val accuracy 0.690, and val rmse 1.585\n",
      "train loss 0.693, val loss 1.266, val accuracy 0.689, and val rmse 1.558\n",
      "train loss 0.701, val loss 1.263, val accuracy 0.693, and val rmse 1.536\n",
      "train loss 0.691, val loss 1.274, val accuracy 0.697, and val rmse 1.554\n",
      "train loss 0.692, val loss 1.278, val accuracy 0.695, and val rmse 1.540\n",
      "train loss 0.685, val loss 1.266, val accuracy 0.698, and val rmse 1.560\n",
      "train loss 0.680, val loss 1.268, val accuracy 0.701, and val rmse 1.549\n",
      "train loss 0.678, val loss 1.263, val accuracy 0.700, and val rmse 1.551\n",
      "train loss 0.664, val loss 1.268, val accuracy 0.702, and val rmse 1.538\n",
      "train loss 0.658, val loss 1.274, val accuracy 0.698, and val rmse 1.545\n",
      "train loss 0.662, val loss 1.267, val accuracy 0.701, and val rmse 1.547\n",
      "train loss 0.661, val loss 1.278, val accuracy 0.701, and val rmse 1.556\n",
      "train loss 0.650, val loss 1.271, val accuracy 0.706, and val rmse 1.546\n",
      "train loss 0.653, val loss 1.274, val accuracy 0.704, and val rmse 1.549\n",
      "train loss 0.647, val loss 1.278, val accuracy 0.704, and val rmse 1.523\n",
      "train loss 0.644, val loss 1.281, val accuracy 0.710, and val rmse 1.520\n",
      "train loss 0.647, val loss 1.284, val accuracy 0.706, and val rmse 1.531\n",
      "train loss 0.650, val loss 1.277, val accuracy 0.708, and val rmse 1.553\n",
      "train loss 0.643, val loss 1.272, val accuracy 0.708, and val rmse 1.538\n",
      "train loss 0.635, val loss 1.264, val accuracy 0.712, and val rmse 1.525\n",
      "train loss 0.629, val loss 1.277, val accuracy 0.710, and val rmse 1.551\n",
      "train loss 0.626, val loss 1.269, val accuracy 0.717, and val rmse 1.497\n",
      "train loss 0.625, val loss 1.276, val accuracy 0.712, and val rmse 1.504\n",
      "train loss 0.629, val loss 1.272, val accuracy 0.713, and val rmse 1.497\n",
      "train loss 0.616, val loss 1.274, val accuracy 0.713, and val rmse 1.531\n",
      "train loss 0.622, val loss 1.288, val accuracy 0.713, and val rmse 1.548\n",
      "train loss 0.618, val loss 1.283, val accuracy 0.714, and val rmse 1.523\n",
      "train loss 0.615, val loss 1.277, val accuracy 0.717, and val rmse 1.521\n",
      "train loss 0.611, val loss 1.274, val accuracy 0.717, and val rmse 1.499\n",
      "train loss 0.599, val loss 1.270, val accuracy 0.722, and val rmse 1.476\n",
      "train loss 0.592, val loss 1.276, val accuracy 0.721, and val rmse 1.498\n",
      "train loss 0.590, val loss 1.279, val accuracy 0.723, and val rmse 1.484\n",
      "train loss 0.590, val loss 1.279, val accuracy 0.724, and val rmse 1.458\n",
      "train loss 0.586, val loss 1.273, val accuracy 0.722, and val rmse 1.492\n",
      "train loss 0.590, val loss 1.279, val accuracy 0.724, and val rmse 1.489\n",
      "train loss 0.588, val loss 1.277, val accuracy 0.721, and val rmse 1.484\n",
      "train loss 0.584, val loss 1.281, val accuracy 0.726, and val rmse 1.493\n",
      "train loss 0.579, val loss 1.294, val accuracy 0.722, and val rmse 1.452\n",
      "train loss 0.577, val loss 1.290, val accuracy 0.723, and val rmse 1.487\n",
      "train loss 0.583, val loss 1.284, val accuracy 0.724, and val rmse 1.485\n",
      "train loss 0.570, val loss 1.287, val accuracy 0.725, and val rmse 1.483\n",
      "train loss 0.575, val loss 1.293, val accuracy 0.726, and val rmse 1.471\n",
      "train loss 0.577, val loss 1.291, val accuracy 0.724, and val rmse 1.471\n",
      "train loss 0.589, val loss 1.283, val accuracy 0.726, and val rmse 1.479\n",
      "train loss 0.581, val loss 1.284, val accuracy 0.724, and val rmse 1.468\n",
      "train loss 0.569, val loss 1.311, val accuracy 0.721, and val rmse 1.520\n",
      "train loss 0.586, val loss 1.295, val accuracy 0.724, and val rmse 1.499\n",
      "train loss 0.585, val loss 1.299, val accuracy 0.721, and val rmse 1.512\n",
      "train loss 0.575, val loss 1.293, val accuracy 0.725, and val rmse 1.490\n",
      "train loss 0.584, val loss 1.300, val accuracy 0.725, and val rmse 1.492\n",
      "train loss 0.579, val loss 1.292, val accuracy 0.724, and val rmse 1.491\n",
      "train loss 0.583, val loss 1.299, val accuracy 0.722, and val rmse 1.494\n",
      "train loss 0.569, val loss 1.311, val accuracy 0.723, and val rmse 1.475\n",
      "train loss 0.574, val loss 1.291, val accuracy 0.725, and val rmse 1.464\n",
      "train loss 0.564, val loss 1.289, val accuracy 0.727, and val rmse 1.480\n",
      "train loss 0.572, val loss 1.292, val accuracy 0.721, and val rmse 1.474\n",
      "train loss 0.579, val loss 1.280, val accuracy 0.730, and val rmse 1.450\n",
      "train loss 0.561, val loss 1.287, val accuracy 0.730, and val rmse 1.463\n",
      "train loss 0.565, val loss 1.285, val accuracy 0.728, and val rmse 1.482\n",
      "train loss 0.555, val loss 1.301, val accuracy 0.727, and val rmse 1.461\n",
      "train loss 0.558, val loss 1.303, val accuracy 0.729, and val rmse 1.455\n",
      "train loss 0.556, val loss 1.307, val accuracy 0.727, and val rmse 1.464\n",
      "train loss 0.544, val loss 1.298, val accuracy 0.730, and val rmse 1.433\n",
      "train loss 0.546, val loss 1.301, val accuracy 0.733, and val rmse 1.438\n",
      "train loss 0.562, val loss 1.292, val accuracy 0.732, and val rmse 1.453\n",
      "train loss 0.551, val loss 1.293, val accuracy 0.733, and val rmse 1.441\n",
      "train loss 0.560, val loss 1.305, val accuracy 0.729, and val rmse 1.489\n",
      "train loss 0.563, val loss 1.314, val accuracy 0.728, and val rmse 1.462\n",
      "train loss 0.562, val loss 1.317, val accuracy 0.726, and val rmse 1.471\n",
      "train loss 0.551, val loss 1.306, val accuracy 0.725, and val rmse 1.474\n",
      "train loss 0.549, val loss 1.302, val accuracy 0.730, and val rmse 1.470\n",
      "train loss 0.555, val loss 1.308, val accuracy 0.727, and val rmse 1.448\n",
      "train loss 0.555, val loss 1.306, val accuracy 0.730, and val rmse 1.444\n",
      "train loss 0.557, val loss 1.310, val accuracy 0.726, and val rmse 1.478\n",
      "train loss 0.544, val loss 1.315, val accuracy 0.730, and val rmse 1.480\n",
      "train loss 0.548, val loss 1.326, val accuracy 0.726, and val rmse 1.516\n",
      "train loss 0.542, val loss 1.325, val accuracy 0.730, and val rmse 1.471\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=128, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "do3FNF0XtI5z"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "correct_by_one = 0\n",
    "correct_by_two = 0\n",
    "total = 0\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        texts, labels, l = data\n",
    "        texts, labels, l = texts.cuda(), labels.cuda(), l.cpu()\n",
    "        texts = texts.long()\n",
    "        outputs = model(texts, l)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        correct_by_one += (predicted == (labels - 1)).sum().item()\n",
    "        correct_by_one += (predicted == (labels + 1)).sum().item()\n",
    "        correct_by_two += (predicted == (labels - 2)).sum().item()\n",
    "        correct_by_two += (predicted == (labels + 2)).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0CGzUAstG1s",
    "outputId": "0dbf0459-d0e7-45ed-881c-2b747c5276a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7352424014066817"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw52rIWDC9pM",
    "outputId": "4160d2f8-3a0d-417f-ce69-55bdde130c86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8399899522732982"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (correct + correct_by_one) / total\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCLreCtuC9rr",
    "outputId": "e3967a76-f1d9-4dce-be53-ffb167581f4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9075609143431299"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (correct + correct_by_one + correct_by_two) / total\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpurv-8MC9uD"
   },
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM for anime reviews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
